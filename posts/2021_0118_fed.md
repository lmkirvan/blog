---
title: A lot of expertise is bullshit. Especially lawyers.  
description: on listening to cass sunstein 
date: 2021-01-18
tags:
  - econ
  - law
  - modeling 
layout: layouts/post.njk
---

The conference was, overall, pretty good. The invited speakers were mostly academic AI and statistics practitioners. And, as with most actual practitioners the hype factor was low. Cynthia Rudin, a interpretable AI computer scientists at Duke, is one interesting example. She has worked to show that their is no trade off between model interpret-ability and accuracy in the vast majority of use cases. A model produced using one of her methods (certifiably optimal rules list) and a widely used criminal justice black box called compass. Her model easily fits, handwritten, on a postcard. It mostly uses age and number of prior offenses to predict the likelihood of re-offending. Compass, by contrast, is proprietary and uses "130 factors" in it's decisions. It is an intentionally "black boxed" model--in that it is hidden from view because of "trade secrets", but the result is the same for models that because of their complexity, cannot be interrogated. We can't know why a decision was made--we can guess, or estimate why, but we can't really know. 

Black boxes make for excellent investor pitches. In the language of VC culture, they create moats around your business that are harder to compete with. Never-mind that most state of art machine learning is open source and essentially fungible. The marketing pitch is more important than the reality, because the goal is less to revolutionize society, than it is to get rich enough to leave society behind. In any event many of the AI practitioners pointed at the ways in which these models are oversold. Or, described approaches that mitigate some of the potential harms, or that demonstrate that, for a given value (such as not discriminating) there are inevitable trade offs with things like predictive accuracy. Much of this insider critical work is obviously small bore. But some does at least grapple with the details in ways that are useful. 

Enter Cass, an undeniable heavyweight of regurgitating behavioral science pablum for consumption by lawyers.  The point of his talk was, I think, to emphasize that, from a decision-making perspective algorithms are "noise free". By this, I take it to mean that, unlike humans, algorithms give you the same decision for a given input. There are lots o things about this analysis that are lacking. For example, the supposedly illuminating example of bail algorithms that "reduce crime" by the worryingly specific amount of 24.8 percent, definitely set off alarm bells. The speech took the complications of such an endeavor as read (for example, do arrests reflect criminal behavior or police behavior?) and briskly moved to celebrating the removal of humans from a process because they aren't consistent. Cash bail as an institution, not worth interrogating. 

There are a lot of things to dislike about this kind of glib but ultimately baleful snake oil intellectual. But what irked me particularly was the laziness. The central idea of the talk was hardly related--indeed the main relationship it had to the proceedings was that the keynote speaker is currently working on a book in that vein. The central thesis of the book is hardly meaningful in the context of models that, like some of the recent language models produced by google, have a trillion parameters. Given that the input space is also infinite (written language), that models are regularly refined and retrained, the insistence on the noiselessness of computer decision-making seems just plain clueless. To be generous the claim is that decision making that takes the form of a function, a one to one mapping of inputs to outputs, is optimal. In models where small perturbations in the input data can result in big changes in predictions, changes that are not easily understood or explained, the fact that if you use the same exact data you will the same again next time hardly seems comforting. *The noise is in the model.* This fact, that you can't explain it, and you can't understand it is central to the problem of inhumane decision-making.